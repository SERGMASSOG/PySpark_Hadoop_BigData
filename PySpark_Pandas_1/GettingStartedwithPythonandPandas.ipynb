{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45683a01",
   "metadata": {},
   "source": [
    "# PySpark y Pandas laboratorio Data Engineering / Data Science\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24043296",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "Este proyecto combina el poder de PySpark y Pandas para abordar tareas de análisis de datos desde dos enfoques complementarios: el procesamiento distribuido a gran escala y la manipulación eficiente en memoria. A través de esta integración, se busca aprovechar lo mejor de ambos mundos para construir flujos de trabajo robustos, escalables y ágiles.\n",
    "- PySpark es la interfaz de Python para Apache Spark, diseñada para ejecutar operaciones sobre grandes volúmenes de datos distribuidos en múltiples nodos. Es ideal para entornos donde los datos superan la capacidad de una sola máquina, como en escenarios de Big Data.\n",
    "- Pandas, por su parte, es una biblioteca especializada en el manejo de datos estructurados en memoria. Ofrece herramientas versátiles para la limpieza, transformación y análisis exploratorio, siendo especialmente útil en etapas de prototipado y validación rápida.\n",
    "\n",
    "## Objectivos\n",
    "- Comprender PySpark y Pandas\n",
    "Dando caso de uso a sus aplicaciones en procesamiento distribuido y manipulación de datos.\n",
    "- Configurar el entorno\n",
    "Instalar y preparar PySpark y Pandas para trabajar de forma conjunta en un entorno Python.\n",
    "- Cargar y explorar datos\n",
    "Importar conjuntos de datos en DataFrames de Pandas y PySpark, y realizar exploraciones básicas para entender su estructura.\n",
    "- Convertir entre DataFrames\n",
    "Transformar un DataFrame de Pandas en uno de PySpark para aprovechar el procesamiento distribuido.\n",
    "- Manipular datos con PySpark\n",
    "Crear columnas nuevas, aplicar filtros y realizar agregaciones utilizando las funciones propias de PySpark.\n",
    "- Ejecutar consultas SQL\n",
    "Utilizar Spark SQL para realizar consultas sobre los datos y aplicar funciones definidas por el usuario (UDFs) para extender la lógica de análisis.\n",
    "- Visualizacion y Mineria de datos\n",
    "Realizar procesamiento y toma de decisiones en funcion de mineria de datos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e424b8",
   "metadata": {},
   "source": [
    "Data -> COVID-19\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586b112a",
   "metadata": {},
   "source": [
    "### Preparacion del entorno\n",
    "Instalacion de librerias necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b46e53fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in c:\\users\\masso\\appdata\\roaming\\python\\python312\\site-packages (4.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in c:\\users\\masso\\appdata\\roaming\\python\\python312\\site-packages (from pyspark) (0.10.9.9)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: findspark in c:\\users\\masso\\appdata\\roaming\\python\\python312\\site-packages (2.0.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\masso\\appdata\\roaming\\python\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\masso\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\masso\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\masso\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\masso\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\masso\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install findspark\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637700c3",
   "metadata": {},
   "source": [
    "## Inicializacion de Spark\n",
    "Configuracion del entorno para empezar a trabajar con Apache Spark desde python usando PySpark y pandas para analisis de los datos. Se debe de preparar una sesion de Spark para poder empezar a procesar los datos.\n",
    "- findspark -> Permite que Python encuentre la instalación de Spark en tu sistema, especialmente útil si Spark no está en el PATH del sistema. \n",
    "- findspark.init() -> Inicializa findspark para que Spark esté disponible / Inicializa findspark para que Spark esté disponible\n",
    "- SparkSession -> Punto de entrada principal para trabajar con DataFrames en PySpark.\n",
    "- SparkSession.builder -> Crea una nueva sesión de Spark.\n",
    "- .appName(\"COVID-19 Data Processing\") -> Asigna un nombre identificador a la aplicación Spark.\n",
    "- .config(...) -> Activa Arrow para mejorar el rendimiento al convertir entre Pandas y PySpark.\n",
    "- .getOrCreate() -> Obtiene una sesión existente o crea una nueva si no hay ninguna activa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f8f4a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession esta activa.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession  \n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DateType # Importa tipos de datos\n",
    "import pandas as pd  \n",
    "findspark.init()\n",
    "# Initialize a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"COVID-19 Data Processing\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Check if the Spark Session is active\n",
    "if 'spark' in locals() and isinstance(spark, SparkSession):\n",
    "    print(\"SparkSession esta activa.\")\n",
    "else:\n",
    "    print(\"SparkSession no esta activa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9cb184",
   "metadata": {},
   "source": [
    "## Lectura de los datos con Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28aaa452",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados correctamente en el DataFrame de Pandas.\n"
     ]
    }
   ],
   "source": [
    "data_covid = pd.read_csv('covid-latest.csv')\n",
    "if data_covid is not None:\n",
    "    print(\"Datos cargados correctamente en el DataFrame de Pandas.\")\n",
    "else:\n",
    "    print(\"Error al cargar los datos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f671a11",
   "metadata": {},
   "source": [
    "## Visualizacion y entendimiento de los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1267201",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mostrando los primeros 5 registros de la base de datos:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso_code</th>\n",
       "      <th>continent</th>\n",
       "      <th>location</th>\n",
       "      <th>last_updated_date</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>new_cases_smoothed</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>new_deaths_smoothed</th>\n",
       "      <th>...</th>\n",
       "      <th>male_smokers</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_thousand</th>\n",
       "      <th>life_expectancy</th>\n",
       "      <th>human_development_index</th>\n",
       "      <th>population</th>\n",
       "      <th>excess_mortality_cumulative_absolute</th>\n",
       "      <th>excess_mortality_cumulative</th>\n",
       "      <th>excess_mortality</th>\n",
       "      <th>excess_mortality_cumulative_per_million</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2024-08-04</td>\n",
       "      <td>235214.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7998.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.50</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>4.112877e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OWID_AFR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Africa</td>\n",
       "      <td>2024-08-04</td>\n",
       "      <td>13145380.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>5.143</td>\n",
       "      <td>259117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.426737e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALB</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2024-08-04</td>\n",
       "      <td>335047.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3605.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>51.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.89</td>\n",
       "      <td>78.57</td>\n",
       "      <td>0.795</td>\n",
       "      <td>2.842318e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DZA</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>2024-08-04</td>\n",
       "      <td>272139.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.571</td>\n",
       "      <td>6881.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30.4</td>\n",
       "      <td>83.741</td>\n",
       "      <td>1.90</td>\n",
       "      <td>76.88</td>\n",
       "      <td>0.748</td>\n",
       "      <td>4.490323e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASM</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>American Samoa</td>\n",
       "      <td>2024-08-04</td>\n",
       "      <td>8359.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.429500e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   iso_code continent        location last_updated_date  total_cases  \\\n",
       "0       AFG      Asia     Afghanistan        2024-08-04     235214.0   \n",
       "1  OWID_AFR       NaN          Africa        2024-08-04   13145380.0   \n",
       "2       ALB    Europe         Albania        2024-08-04     335047.0   \n",
       "3       DZA    Africa         Algeria        2024-08-04     272139.0   \n",
       "4       ASM   Oceania  American Samoa        2024-08-04       8359.0   \n",
       "\n",
       "   new_cases  new_cases_smoothed  total_deaths  new_deaths  \\\n",
       "0        0.0               0.000        7998.0         0.0   \n",
       "1       36.0               5.143      259117.0         0.0   \n",
       "2        0.0               0.000        3605.0         0.0   \n",
       "3       18.0               2.571        6881.0         0.0   \n",
       "4        0.0               0.000          34.0         0.0   \n",
       "\n",
       "   new_deaths_smoothed  ...  male_smokers  handwashing_facilities  \\\n",
       "0                  0.0  ...           NaN                  37.746   \n",
       "1                  0.0  ...           NaN                     NaN   \n",
       "2                  0.0  ...          51.2                     NaN   \n",
       "3                  0.0  ...          30.4                  83.741   \n",
       "4                  0.0  ...           NaN                     NaN   \n",
       "\n",
       "   hospital_beds_per_thousand  life_expectancy  human_development_index  \\\n",
       "0                        0.50            64.83                    0.511   \n",
       "1                         NaN              NaN                      NaN   \n",
       "2                        2.89            78.57                    0.795   \n",
       "3                        1.90            76.88                    0.748   \n",
       "4                         NaN            73.74                      NaN   \n",
       "\n",
       "     population  excess_mortality_cumulative_absolute  \\\n",
       "0  4.112877e+07                                   NaN   \n",
       "1  1.426737e+09                                   NaN   \n",
       "2  2.842318e+06                                   NaN   \n",
       "3  4.490323e+07                                   NaN   \n",
       "4  4.429500e+04                                   NaN   \n",
       "\n",
       "   excess_mortality_cumulative  excess_mortality  \\\n",
       "0                          NaN               NaN   \n",
       "1                          NaN               NaN   \n",
       "2                          NaN               NaN   \n",
       "3                          NaN               NaN   \n",
       "4                          NaN               NaN   \n",
       "\n",
       "   excess_mortality_cumulative_per_million  \n",
       "0                                      NaN  \n",
       "1                                      NaN  \n",
       "2                                      NaN  \n",
       "3                                      NaN  \n",
       "4                                      NaN  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mostrando los primeros 5 registros de la base de datos:\")\n",
    "# Funcion para mostrar las primeras 5 filas del DataFrame\n",
    "data_covid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d9256",
   "metadata": {},
   "source": [
    "## Pandas DataFrame a Spark DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7999fb9-d772-439e-b3c4-bf3d1cc65553",
   "metadata": {},
   "source": [
    "- The resulting spark_df will have the defined schema, which ensures consistency and compatibility with Spark's data processing capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1411dfe1-382b-4edb-9d82-c8175e4c3df6",
   "metadata": {},
   "source": [
    "### Storing the result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdde4c4-13b3-43b3-a351-2b082d1d9c0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------+------------------+----------+\n",
      "|    continent|total_cases|total_deaths|total_vaccinations|population|\n",
      "+-------------+-----------+------------+------------------+----------+\n",
      "|         Asia|     235214|        7998|                 0|  41128772|\n",
      "|          nan|   13145380|      259117|                 0|1426736614|\n",
      "|       Europe|     335047|        3605|                 0|   2842318|\n",
      "|       Africa|     272139|        6881|                 0|  44903228|\n",
      "|      Oceania|       8359|          34|                 0|     44295|\n",
      "|       Europe|      48015|         159|                 0|     79843|\n",
      "|       Africa|     107481|        1937|                 0|  35588996|\n",
      "|North America|       3904|          12|                 0|     15877|\n",
      "|North America|       9106|         146|                 0|     93772|\n",
      "|South America|   10101218|      130663|                 0|  45510324|\n",
      "|         Asia|     452273|        8777|                 0|   2780472|\n",
      "|North America|      44224|         292|                 0|    106459|\n",
      "|          nan|  301499099|     1637249|        9104304615|4721383370|\n",
      "|      Oceania|   11861161|       25236|                 0|  26177410|\n",
      "|       Europe|    6082444|       22534|                 0|   8939617|\n",
      "|         Asia|     835757|       10353|                 0|  10358078|\n",
      "|North America|      39127|         849|                 0|    409989|\n",
      "|         Asia|     696614|        1536|                 0|   1472237|\n",
      "|         Asia|    2051348|       29499|                 0| 171186368|\n",
      "|North America|     108582|         593|                 0|    281646|\n",
      "+-------------+-----------+------------+------------------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Pasando a Spark DataFrame\n",
    "# Definiendo el schema\n",
    "schema = StructType([\n",
    "    StructField(\"continent\", StringType(), True),\n",
    "    StructField(\"total_cases\", LongType(), True),\n",
    "    StructField(\"total_deaths\", LongType(), True),\n",
    "    StructField(\"total_vaccinations\", LongType(), True),\n",
    "    StructField(\"population\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Pasando el DataFrame de Pandas a Spark DataFrame con el schema definido\n",
    "# Asegurando que los tipos de datos coincidan con el schema\n",
    "data_covid['continent'] = data_covid['continent'].astype(str)  # Ensures continent is a string\n",
    "data_covid['total_cases'] = data_covid['total_cases'].fillna(0).astype('int64')  # Fill NaNs and convert to int\n",
    "data_covid['total_deaths'] = data_covid['total_deaths'].fillna(0).astype('int64')  # Fill NaNs and convert to int\n",
    "data_covid['total_vaccinations'] = data_covid['total_vaccinations'].fillna(0).astype('int64')  # Fill NaNs and convert to int\n",
    "data_covid['population'] = data_covid['population'].fillna(0).astype('int64')  # Fill NaNs and convert to int\n",
    "\n",
    "spark_df = spark.createDataFrame(data_covid[schema.fieldNames()])  # Usando solo las columnas definidas en el schema\n",
    "# Mostrando las primeras 5 filas del Spark DataFrame\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8efcae4",
   "metadata": {},
   "source": [
    "## Mostrando la estructura\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555080e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the Spark DataFrame:\n",
      "root\n",
      " |-- continent: string (nullable = true)\n",
      " |-- total_cases: long (nullable = true)\n",
      " |-- total_deaths: long (nullable = true)\n",
      " |-- total_vaccinations: long (nullable = true)\n",
      " |-- population: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Schema of the Spark DataFrame:\")\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b3d525",
   "metadata": {},
   "source": [
    "## EDA - Exploracion de los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e31c016e-8623-47f7-8562-7c15254e1109",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------+------------------+----------+\n",
      "|continent|total_cases|total_deaths|total_vaccinations|population|\n",
      "+---------+-----------+------------+------------------+----------+\n",
      "|     Asia|     235214|        7998|                 0|  41128772|\n",
      "|      nan|   13145380|      259117|                 0|1426736614|\n",
      "|   Europe|     335047|        3605|                 0|   2842318|\n",
      "|   Africa|     272139|        6881|                 0|  44903228|\n",
      "|  Oceania|       8359|          34|                 0|     44295|\n",
      "+---------+-----------+------------+------------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Lista de columnas a mostrar\n",
    "columns_to_display = ['continent', 'total_cases', 'total_deaths', 'total_vaccinations', 'population']\n",
    "spark_df.select(columns_to_display).show(5) #\" Mostrar las primeras 5 filas de las columnas seleccionadas\" con Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb630c-52a0-4448-bb49-6d217195a169",
   "metadata": {},
   "source": [
    "### 9.2 Picking specific columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2cd6146-c8a8-4761-8b46-8d10e48bb29c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mostrando el total de casos por el continente:\n",
      "+-------------+-----------+\n",
      "|    continent|total_cases|\n",
      "+-------------+-----------+\n",
      "|         Asia|     235214|\n",
      "|          nan|   13145380|\n",
      "|       Europe|     335047|\n",
      "|       Africa|     272139|\n",
      "|      Oceania|       8359|\n",
      "|       Europe|      48015|\n",
      "|       Africa|     107481|\n",
      "|North America|       3904|\n",
      "|North America|       9106|\n",
      "|South America|   10101218|\n",
      "+-------------+-----------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Mostrando el total de casos por el continente:\")\n",
    "spark_df.select('continent', 'total_cases').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "864c3317-aaf2-4690-8f95-aa5bf6568c14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtrando registros con más de 3 millónes de casos:\n",
      "+-------------+-----------+------------+------------------+----------+\n",
      "|    continent|total_cases|total_deaths|total_vaccinations|population|\n",
      "+-------------+-----------+------------+------------------+----------+\n",
      "|          nan|   13145380|      259117|                 0|1426736614|\n",
      "|South America|   10101218|      130663|                 0|  45510324|\n",
      "|          nan|  301499099|     1637249|        9104304615|4721383370|\n",
      "|      Oceania|   11861161|       25236|                 0|  26177410|\n",
      "|       Europe|    6082444|       22534|                 0|   8939617|\n",
      "|       Europe|    4872829|       34339|                 0|  11655923|\n",
      "|South America|   37511921|      702116|                 0| 215313504|\n",
      "|North America|    4819055|       55282|         102877159|  38454328|\n",
      "|South America|    5401126|       62730|                 0|  19603736|\n",
      "|         Asia|   99373219|      122304|                 0|1425887360|\n",
      "+-------------+-----------+------------+------------------+----------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtrando registros con más de 3 millónes de casos:\")\n",
    "spark_df.filter(spark_df['total_cases'] > 3000000).show(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cb2722",
   "metadata": {},
   "source": [
    "## Modificacion y creacion de nuevas columnas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4cd634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------------+---------+------------------+-----------+\n",
      "|total_deaths|population|death_percentage|continent|total_vaccinations|total_cases|\n",
      "+------------+----------+----------------+---------+------------------+-----------+\n",
      "|        7998|  41128772|           0.02%|     Asia|                 0|     235214|\n",
      "|      259117|1426736614|           0.02%|      nan|                 0|   13145380|\n",
      "|        3605|   2842318|           0.13%|   Europe|                 0|     335047|\n",
      "|        6881|  44903228|           0.02%|   Africa|                 0|     272139|\n",
      "|          34|     44295|           0.08%|  Oceania|                 0|       8359|\n",
      "+------------+----------+----------------+---------+------------------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark_df_with_percentage = spark_df.withColumn(\n",
    "    'death_percentage', \n",
    "    (spark_df['total_deaths'] / spark_df['population']) * 100\n",
    ")\n",
    "spark_df_with_percentage = spark_df_with_percentage.withColumn(\n",
    "    'death_percentage',\n",
    "    F.concat(\n",
    "        # Aplicando formato de número con 2 decimales\n",
    "        F.format_number(spark_df_with_percentage['death_percentage'], 2), \n",
    "        # Incluyendo el símbolo de porcentaje\n",
    "        F.lit('%')  \n",
    "    )\n",
    ")\n",
    "columns_to_display = ['total_deaths', 'population', 'death_percentage', 'continent', 'total_vaccinations', 'total_cases']\n",
    "spark_df_with_percentage.select(columns_to_display).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e317b1d6",
   "metadata": {},
   "source": [
    "## Agrupacion y sumatorias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed954475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando el total de muertes por continente:\n",
      "+-------------+-----------------+\n",
      "|    continent|sum(total_deaths)|\n",
      "+-------------+-----------------+\n",
      "|       Europe|          2102483|\n",
      "|       Africa|           259117|\n",
      "|          nan|         22430618|\n",
      "|North America|          1671178|\n",
      "|South America|          1354187|\n",
      "|      Oceania|            32918|\n",
      "|         Asia|          1637249|\n",
      "+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculando el total de muertes por continente:\")\n",
    "spark_df.groupby(['continent']).agg({\"total_deaths\": \"SUM\"}).show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee06742",
   "metadata": {},
   "source": [
    "## UDFs\n",
    "- Las UDFs (User Defined Functions) en PySpark son funciones personalizadas que tú defines en Python para aplicar transformaciones sobre columnas de un DataFrame distribuido. Son útiles cuando necesitas lógica que no está disponible en las funciones nativas de Spark.\n",
    "- Spark tiene muchas funciones integradas (withColumn, filter, groupBy, etc.), pero a veces necesitas aplicar una lógica específica que no existe. Ahí es donde entran las UDFs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "724fa2ec-6a45-4f26-9555-040e129b3dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.convert_total_deaths(x)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "# Creacion de funcion UDF\n",
    "\n",
    "def convert_total_deaths(x):\n",
    "    return int(x) * 2 if x is not None else None\n",
    "\n",
    "spark.udf.register(\"convert_total_deaths\", convert_total_deaths, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2173a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark_df = spark_df.withColumn(\"total_deaths\", col(\"total_deaths\").cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40388f73",
   "metadata": {},
   "source": [
    "## Spark SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86424dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[continent: string, total_deaths: int, converted_total_deaths: int]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Borrar la tabla temporal si ya existe\n",
    "spark.sql(\"DROP VIEW IF EXISTS data_v\")\n",
    "# Crear de nuevo una vista temporal\n",
    "spark_df.createOrReplaceTempView(\"data_v\")\n",
    "# Ejecutamos la consulta SQL usando la UDF\n",
    "spark.sql(\"\"\"\n",
    "    SELECT continent, total_deaths, convert_total_deaths(total_deaths) AS converted_total_deaths\n",
    "    FROM data_v\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0860017d",
   "metadata": {},
   "source": [
    "SQL queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bfd78c13-a5c8-4d06-a0bb-bfb85225db3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------+------------------+----------+\n",
      "|    continent|total_cases|total_deaths|total_vaccinations|population|\n",
      "+-------------+-----------+------------+------------------+----------+\n",
      "|         Asia|     235214|        7998|                 0|  41128772|\n",
      "|          nan|   13145380|      259117|                 0|1426736614|\n",
      "|       Europe|     335047|        3605|                 0|   2842318|\n",
      "|       Africa|     272139|        6881|                 0|  44903228|\n",
      "|      Oceania|       8359|          34|                 0|     44295|\n",
      "|       Europe|      48015|         159|                 0|     79843|\n",
      "|       Africa|     107481|        1937|                 0|  35588996|\n",
      "|North America|       3904|          12|                 0|     15877|\n",
      "|North America|       9106|         146|                 0|     93772|\n",
      "|South America|   10101218|      130663|                 0|  45510324|\n",
      "|         Asia|     452273|        8777|                 0|   2780472|\n",
      "|North America|      44224|         292|                 0|    106459|\n",
      "|          nan|  301499099|     1637249|        9104304615|4721383370|\n",
      "|      Oceania|   11861161|       25236|                 0|  26177410|\n",
      "|       Europe|    6082444|       22534|                 0|   8939617|\n",
      "|         Asia|     835757|       10353|                 0|  10358078|\n",
      "|North America|      39127|         849|                 0|    409989|\n",
      "|         Asia|     696614|        1536|                 0|   1472237|\n",
      "|         Asia|    2051348|       29499|                 0| 171186368|\n",
      "|North America|     108582|         593|                 0|    281646|\n",
      "+-------------+-----------+------------+------------------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM data_v').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a146173a-0b9f-477b-b81c-552309c52e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    continent|\n",
      "+-------------+\n",
      "|          nan|\n",
      "|North America|\n",
      "|       Europe|\n",
      "|       Europe|\n",
      "|          nan|\n",
      "|          nan|\n",
      "|          nan|\n",
      "|         Asia|\n",
      "|         Asia|\n",
      "|       Europe|\n",
      "|          nan|\n",
      "|         Asia|\n",
      "|      Oceania|\n",
      "|          nan|\n",
      "|          nan|\n",
      "|          nan|\n",
      "|          nan|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL filtering\n",
    "spark.sql(\"SELECT continent FROM data_v WHERE total_vaccinations > 1000000\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "prev_pub_hash": "45a0dd703bfbcc37ede89eec12aa2a3ff03ef47840aa569cd67704c66ddb0f6c"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
